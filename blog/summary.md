## 关于深度学习

### 几个要点
**神经网络**
- 误差反向传播的各层计算方法
- 每一层都要设置一个激活函数

**训练方法**
- 优化方法不仅仅有SGD，还有Momentum， AdaGrad，Adam等
- 一般训练时都用mini-batch小批量训练，当batch_size * iternum = input_size时，即所有训练数据都使用过后为一个epoch
- 对每一个epoch计算其精度

**初始值设置**
- Sigmoid和tanh初始值使用Xavier初始值， 使用标准差为$\sqrt{\frac{1}{n}}$， 均值为0的正态分布
- ReLU初始值使用He初始值，当前一层的节点数为n， 使用标准差为$\sqrt{\frac{2}{n}}$，均值为0的正态分布

**抑制过拟合**
- *batch normalization*
- 对mini-batch进行正规化
- ![image](https://user-images.githubusercontent.com/91414286/191686628-b9035034-8d75-410b-9978-a5a0e9010055.png)

- *正则化*
- 损失函数上加上L2正则化项

- *Dropout*
- 随机删除神经元

**超参数的验证**
- 训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。
- 将数据集分为训练数据、验证数据、测试数据三部分
- 自行分割数据集则训练数据的20%为验证数据
- 超参数的范围只要“大致地指定”就可以了。所谓“大致地指定”，是指像0.001（10−3）到1000（103）这样，最后找到一个合适的值
