{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "635884e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 271>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(W):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m net\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m--> 271\u001b[0m \u001b[43mnum_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m net\u001b[38;5;241m.\u001b[39mw\n",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36mnum_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m    226\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(x)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grad\u001b[38;5;241m.\u001b[39msize):\n\u001b[1;32m--> 228\u001b[0m     grad[i] \u001b[38;5;241m=\u001b[39m num_center_differential(f, \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "#sys.path.append(os.pardir) # 为了导入父目录中的文件而进行的设定\n",
    "\n",
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "#阶跃函数\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=int)\n",
    "\n",
    "#sigmoid函数 f(x) = 1 / (1 + e ** -x)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#ReLU\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "#激活函数\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "#定义softmax函数\n",
    "#def softmax(x):\n",
    "#    return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "#改进softmax函数\n",
    "def softmax(x):\n",
    "    dim = x.shape[0]\n",
    "    if x.ndim == 1:\n",
    "        x_max = np.max(x)\n",
    "        return np.exp(x - x_max) / np.exp(x - x_max).sum()\n",
    "    if x.ndim == 2:\n",
    "        x_max = np.max(x, axis = 1)\n",
    "        return np.exp(x - x_max.reshape(dim, -1)) / np.exp(x - x_max.reshape(dim, -1)).sum(axis = 1).reshape(dim, -1)\n",
    "\n",
    "#x = np.arange(-5, 5, 0.1)\n",
    "#plt.plot(x, sigmoid(x), label = 'sigmoid')\n",
    "#plt.plot(x, step_function(x), linestyle = '--', label = 'step_function')\n",
    "#plt.plot(x, ReLU(x), linestyle = '-', label = 'ReLU')\n",
    "#plt.xlabel(\"x\") # x轴标签\n",
    "#plt.ylabel(\"y\") # y轴标签\n",
    "#plt.title('sin & cos') # 标题\n",
    "#plt.legend()\n",
    "##plt.ylim(0, 1)\n",
    "#plt.show()\n",
    "\n",
    "#x = np.arange(-5, 5, 0.1)\n",
    "#plt.plot(x, ReLU(x), linestyle = '-')\n",
    "#plt.show()\n",
    "\n",
    "# 输出层的激活函数用σ() 表示，不同于隐藏层的激活函数h()（σ读作sigma）。\n",
    "#X = np.array([1.0, 0.5])\n",
    "#W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "#B1 = np.array([0.1, 0.2, 0.3])\n",
    "#A1 = np.dot(X, W1) + B1\n",
    "#Z1 = sigmoid(A1)\n",
    "#W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "#B2 = np.array([0.1, 0.2])\n",
    "#A2 = np.dot(Z1, W2) + B2\n",
    "#Z2 = sigmoid(A2)\n",
    "#W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "#B3 = np.array([0.1, 0.2])\n",
    "#A3 = np.dot(Z2, W3) + B3\n",
    "#Y = identity_function(A3)\n",
    "\n",
    "\n",
    "#人工定义神经网络初始化参数\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    Z1 = sigmoid(np.dot(x, W1) + b1)\n",
    "    Z2 = sigmoid(np.dot(Z1, W2) + b2)\n",
    "    y = identity_function(np.dot(Z2, W3) + b3)\n",
    "    return y\n",
    "\n",
    "#x = np.array([1.0, 0.5])\n",
    "#y = forward(init_network(), x)\n",
    "\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "    \n",
    "    \n",
    "#显示图片和表情\n",
    "#img = x_train[0]\n",
    "#label = t_train[0]\n",
    "#img = img.reshape(28, -1)\n",
    "#print(label)\n",
    "#img.shape\n",
    "#img_show(img)\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open('.\\\\ch03\\\\sample_weight.pkl', 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "#随机梯度下降\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    Z1 = sigmoid(np.dot(x, W1) + b1)\n",
    "    Z2 = sigmoid(np.dot(Z1, W2) + b2)\n",
    "    y = softmax(np.dot(Z2, W3) + b3)\n",
    "    return y\n",
    "\n",
    "#x, t = get_data()\n",
    "#network = init_network()\n",
    "#accuracy_cnt = 0\n",
    "#for i in range(len(x)):\n",
    "#    y = predict(network, x[i])\n",
    "#    p = np.argmax(y) # 获取概率最高的元素的索引\n",
    "#    if p == t[i]:\n",
    "#        accuracy_cnt += 1\n",
    "#print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n",
    "\n",
    "\n",
    "#批量梯度下降（batch）\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i: i + batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis = 1)\n",
    "    accuracy_cnt += np.sum(p == t[i: i + batch_size])\n",
    "\n",
    "#均方误差，输入numpy数组\n",
    "def mean_squared_error(y_pre, y):\n",
    "    return 0.5 * np.sum((y - y_pre) ** 2)\n",
    "\n",
    "#交叉熵，输入numpy数组\n",
    "def cross_entropy(y_pre, y):\n",
    "    delta = 1e-7\n",
    "    return - np.sum(y_pre * np.log(y + delta))\n",
    "\n",
    "#mini-batch算法，每次选出一小部分作为整体的平均代表\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "#改良交叉熵\n",
    "def cross_entropy_2(y_pre, y):\n",
    "    if y.ndim != 1:\n",
    "        y = y.reshape(-1)\n",
    "        y_pre = y_pre.reshape(-1)\n",
    "    delta = 1e-7\n",
    "    return (-np.sum(y_pre * np.log(y + delta))) / y.shape[0]\n",
    "\n",
    "#前向差分\n",
    "def num_differential(f, x):\n",
    "    delta_x = 1e-20\n",
    "    return (f(x + delta_x) - f(x)) / delta_x\n",
    "\n",
    "#中心差分\n",
    "def num_center_differential(f, x):\n",
    "    h = 1e-10\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def fn(x):\n",
    "    return 0.01 * x * x + 0.1 * x\n",
    "\n",
    "def line(f, x):\n",
    "    y_diff = num_center_differential(f, x)\n",
    "    y_0 = f(x) - y_diff * x\n",
    "    return lambda t: y_diff * t + y_0\n",
    "\n",
    "#微分图\n",
    "#x = np.arange(0, 20, 0.1)\n",
    "#y = fn(x)\n",
    "#plt.plot(x, y)\n",
    "#plt.xlabel('x')\n",
    "#plt.ylabel('f(x)')\n",
    "#lf = line(fn, 10)\n",
    "#y2 = lf(x)\n",
    "#plt.plot(x, y2)\n",
    "#plt.show()\n",
    "\n",
    "#偏导数实现\n",
    "def fn_2(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def fn_partial1(x):\n",
    "    return x * x + 4 * 4\n",
    "\n",
    "def fn_partial2(x):\n",
    "    return 3 * 3 + x * x\n",
    "\n",
    "#梯度实现\n",
    "def num_gradient(f, x):\n",
    "    #h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(grad.size):\n",
    "        grad[i] = num_center_differential(f, x[i])\n",
    "    return grad\n",
    "\n",
    "#num_gradient(fn_2, np.array([3, 0, 6]))\n",
    "\n",
    "#梯度下降实现\n",
    "def gradient_descent(loss, x_init, lr = 0.01, step_num = 100):\n",
    "    x = x_init\n",
    "    for i in range(step_num):\n",
    "        grad = num_gradient(loss, x)\n",
    "        x = x - lr * grad\n",
    "    return x\n",
    "\n",
    "#实例\n",
    "x_init = np.array([-3, 4])\n",
    "gradient_descent(fn_2, x_init, lr = 0.2)\n",
    "\n",
    "#构建神经网络类\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.w)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = softmax(self.predict(x))\n",
    "        loss = cross_entropy_2(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def num_gradient(self, x, t):\n",
    "        return \n",
    "\n",
    "    \n",
    "#测试\n",
    "net = SimpleNet()\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "num_gradient(f, net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9faed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2, 3)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl] *",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
